import argparse
import datetime
import json
import logging
import re
import sqlite3
import time
from dataclasses import dataclass
from math import isnan
import os

import pandas as pd
from cvss import CVSS2, CVSS3, CVSS4

"""
================================== GLOBALS =================================
"""

logger = logging.getLogger(__name__)
logging.basicConfig(filename='../get_cves.log', encoding='utf-8', format='%(asctime)s::%(levelname)s::%(message)s',
                    level=logging.INFO)

con = sqlite3.connect('../cve3.db')  # db connection
cur = con.cursor() 

vendor_map = None  # vendor names found using snmp differ from those in the db, so we define a mapping between them


@dataclass
class Env(argparse.Namespace):
    input: str
    # Initially, some vendor names from the snmp scans won't be in the `vendor_mapping.json`. First do a dry run,
    # which will report any missing mappings without doing any db operations. After you're sure all necessary mappings
    # were added, re-run the script for real
    dry_run: bool
    severity: float
    routers_only: bool
    surf: bool
    iberia: bool
    explain_query: bool
    precomputed_table: bool
    input_tags: str


# get CLI args
parser = argparse.ArgumentParser()
parser.add_argument('-i', '--input', type=str, default='scan_results', help='path to the input csv file')
parser.add_argument('-n', '--dry-run', action='store_true', help='only log unknown vendors')
parser.add_argument('-s', '--severity', type=float, default=None,
                    help='include severity metrics with severity larger than the provided value in the output')
parser.add_argument('-r', '--routers-only', action='store_true', help='only report CVEs related to routers')
parser.add_argument('--surf', action='store_true', help='keep only results from surf.nl')
parser.add_argument('-q', '--explain-query', action='store_true', help='explain the query that is being run')
parser.add_argument('-p', '--precomputed-table', action='store_true', help='use the precomputed table; less flexible but much faster')
parser.add_argument('--iberia', action='store_true', help='keep only results from the Iberian peninsula')
parser.add_argument('-it', '--input-tags', type=str, default='../bgp-tools/csvs', help='path to directory containing tags for asns')

env = Env(**vars(parser.parse_args()))

"""
=================================== METHODS ==================================
"""

def vendor_resolution(enterprise: str) -> str | None:
    """
    Resolve the vendor shortname given the enterprise name obtained from the scanning. The map is stored in 
    `vendor_mapping.json`. 
    :param enterprise: the `enterprise` field from the scanning results
    :return: the shortname of the vendor, as found in the db
    :raises KeyError: if the enterprise is not found in the mapping. Should be manually added to the mapping file
    """
    global vendor_map
    if vendor_map is None:  # load it once
        with open('../vendor_mapping.json', 'r') as f:
            vendor_map = json.load(f)
            
    if isnan(enterprise):
        return None
    
    enterprise = str(int(enterprise)) # they are parsed as floats for some reason
    
    if enterprise in vendor_map:
        return vendor_map[enterprise]

    # logged
    logger.warning(f"Enterprise {enterprise} not found in vendor map. Add it to vendor_mapping.json.")
    return None


def convert_to_date(time: str) -> datetime.date:
    """
    Calculates the date the last boot took place from the snmpEngineTime field.
    :param time: the value of the snmpEngineTime field
    :return: the `datetime.date` representing the date of the last boot
    """
    try:
        days_since_reboot = re.search(r'(\d+)d', str(time)).group(1)
    except AttributeError: # if reboot is too recent, they won't have a day field. We can assume it is 0
        days_since_reboot = 0

    reboot_date = datetime.date.today() - datetime.timedelta(days=int(days_since_reboot))

    return reboot_date.strftime('%Y-%m-%dT%H:%M:%S')


def build_query(query_input: list[tuple[str, str, str, str]]) -> str:
    conditions = ['m.datePublished > d.column2', 'm.shortVendorName = d.column3', ]
    selections = ['m.cveId', 'm.shortVendorName', 'd.column1 as engineId', 'd.column4 as engineBoots', 'd.column2 as rebootDate', 'd.column5 as asn']
    
    if env.precomputed_table:
        table = 'precomputed_table'
    else:
        table = 'cve_metadata NATURAL JOIN cve_vendor NATURAL JOIN vendors'
    
    # build selected columns
    if env.severity is not None:
        selections.append('m.baseScore')
        selections.append('m.exploitabilityScore')
    
    # build conditions
    if env.routers_only: 
        # 0 = for sure related; 
        # 1 = did not have a description, so not sure; 
        # 2 = could not find keywords in the description
        conditions.append('m.relatedToRouters < 2')

    if env.severity is not None:
        conditions.append(f'm.baseScore >= {env.severity}')

    full_input = ', '.join(['(' + ', '.join([f'\'{entry}\'' for entry in value]) + ')' for value in query_input])
        
    return f"""
       WITH date_assigner_pairs AS (VALUES {full_input})
        SELECT {', '.join(selections)}
        FROM date_assigner_pairs d
        JOIN (SELECT * FROM {table}) m ON {' AND '.join(conditions)}
    """


def find_cves(scans: pd.DataFrame) -> pd.DataFrame | None:
    """
    Queries the database for any CVEs for the vendor that were published after the date of the last boot.
    :param scans: a row like in the input csv
    :param engineid_cve_mapping: mutable dictionary that is updated with the found CVEs for the engineId
    :return: nothing, it updates the dictionary instead
    """
    engineIds = scans['engineIDData'].to_list()
    engineBoots = scans['snmpEngineBoots'].to_list()
    vendor_shortname = scans['enterprise'].apply(vendor_resolution).to_list()
    reboot_date = scans['snmpEngineTime'].apply(convert_to_date).to_list()
    asns = scans['asn'].to_list()

    if env.dry_run:
        return None

    values: list[tuple[str, str, str, str, str]] = [tup for tup in zip(engineIds, reboot_date, vendor_shortname, engineBoots, asns) if tup[2] is not None]
    
    query = build_query(values)

    if env.explain_query:
        print(con.execute(f'EXPLAIN QUERY PLAN {query}').fetchall())
        print(query)
        
    print('Starting query')
    start = time.time()
    cve_list = (pd.read_sql_query(query, con))
    print(f'Queried and processed {len(engineIds)} items in {time.time() - start:.2f} seconds')

    return cve_list


def load_input() -> pd.DataFrame:
    """
    Reads all the scan results in the folder given as an argument into a single dataframe.
    :return: The dataframe with all the scan results.
    """
    import os
    files = [pd.read_csv(os.path.join(env.input, file)) for file in os.listdir(env.input)]
    
    return pd.concat(files).reset_index()


def load_tags(dir: str) -> pd.DataFrame | None :
    try:

        if not os.path.isdir(dir):
            print(f"dir {dir} doesn't exist")
            return None

        dfs = []

        for filename in os.listdir(dir):
            if filename.endswith('.csv') and filename != "feeder.csv":
                filepath = os.path.join(dir, filename)
                try:
                    df = pd.read_csv(filepath, header=None, names=['ASN', 'Organization'])
                    df['source_file'] = filename.split(".")[0]
                    df = df.iloc[:, [2, 1, 0]]
                    df['ASN'] = df['ASN'].astype(str).str.upper().str.removeprefix("AS")
                    dfs.append(df)

                except Exception as e:
                    print(f"{filename} failed; error: {e}")

        if dfs:
            res = pd.concat(dfs, ignore_index=True)
            res.columns = ["Tag", "Organization Name", "ASN"]
            res.to_csv(r'tags.csv', index=None, sep=';', mode='w')
            return res
        else:
            print("No CSV files found.")
            return None
    except Exception as e:
        print(f"LOL{str(e)}")

if __name__ == '__main__':
    df = load_input()
    
    if env.surf:
        df = df.loc[(df['asn_name'] == 'SURF B.V.') & (df['country'] == 'The Netherlands')]
        
    if env.iberia:
        df = df.loc[(df['country'] == 'Spain') | (df['country'] == 'Portugal')]

    engineid_cve_mapping: pd.DataFrame = find_cves(df)

    # dump results to a JSON file
    engineid_cve_mapping.to_csv('../found_cves.csv')

    if env.input_tags:
        print("adding tags to output")
        tags_df = load_tags(env.input_tags)

        tags_df.to_csv('tags.csv', index=False)

        original_length = len(engineid_cve_mapping)

        if tags_df is not None and engineid_cve_mapping is not None:
            tags_df['ASN'] = tags_df['ASN'].astype(str).str.strip()
            engineid_cve_mapping['asn'] = engineid_cve_mapping['asn'].astype(str).str.strip()


            print("aggregating tags per asn")
            tags_agg = tags_df.groupby('ASN')['Tag'].apply(lambda x: ';'.join(sorted(set(x)))).reset_index()

            print("merging aggregated tags")

            merged = pd.merge(
                engineid_cve_mapping,
                tags_agg,
                left_on='asn',
                right_on='ASN',
                how='left'
            ).drop(columns=['ASN'])

            merged['Tag'] = merged['Tag'].fillna('UNKNOWN')

            merged.rename(columns={'Tag': 'tag'}, inplace=True)

            columns_order = ['cveId', 'shortVendorName', 'engineId', 'engineBoots', 'rebootDate', 'asn', 'tag']
            if 'baseScore' in merged.columns:
                columns_order += ['baseScore', 'exploitabilityScore']
            merged = merged[columns_order]
                        
            output_with_tags_path = '../found_cves_with_tags.csv'
            merged.to_csv(output_with_tags_path, index=False)

            print(f"Saved results with tags to {output_with_tags_path}")
            print(f"Original CVE rows: {original_length}")
            print(f"Final tagged rows: {len(merged)}")