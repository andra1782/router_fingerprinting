import argparse
import datetime
import json
import logging
import re
import sqlite3
import time
from dataclasses import dataclass
from math import isnan

import pandas as pd
from cvss import CVSS2, CVSS3, CVSS4

"""
============================= GLOBALS ============================
"""

logger = logging.getLogger(__name__)
logging.basicConfig(filename='../get_cves.log', encoding='utf-8', format='%(asctime)s::%(levelname)s::%(message)s',
                    level=logging.INFO)

con = sqlite3.connect('../cve3.db')  # db connection
cur = con.cursor() 

vendor_map = None  # vendor names found using snmp differ from those in the db, so we define a mapping between them


@dataclass
class Env(argparse.Namespace):
    input: str
    # Initially, some vendor names from the snmp scans won't be in the `vendor_mapping.json`. First do a dry run,
    # which will report any missing mappings without doing any db operations. After you're sure all necessary mappings
    # were added, re-run the script for real
    dry_run: bool
    severity: float
    routers_only: bool
    verbose_severity: bool
    surf: bool
    explain_query: bool


# get CLI args
parser = argparse.ArgumentParser()
parser.add_argument('-i', '--input', type=str, default='scan_results', help='path to the input csv file')
parser.add_argument('-n', '--dry-run', action='store_true', help='only log unknown vendors')
parser.add_argument('-s', '--severity', type=float, default=None,
                    help='include severity metrics with severity larger than the provided value in the output')
parser.add_argument('-r', '--routers-only', action='store_true', help='only report CVEs related to routers')
parser.add_argument('-vs', '--verbose-severity', action='store_true',
                    help='increase verbosity of severity metrics by extracting the information encoded in the cvss vector string')
parser.add_argument('--surf', action='store_true', help='keep only results from surf.nl')
parser.add_argument('-q', '--explain-query', action='store_true', help='explain the query that is being run')
env = Env(**vars(parser.parse_args()))

"""
============================== METHODS =============================
"""

def vendor_resolution(enterprise: str) -> str | None:
    """
    Resolve the vendor shortname given the enterprise name obtained from the scanning. The map is stored in 
    `vendor_mapping.json`. 
    :param enterprise: the `enterprise` field from the scanning results
    :return: the shortname of the vendor, as found in the db
    :raises KeyError: if the enterprise is not found in the mapping. Should be manually added to the mapping file
    """
    global vendor_map
    if vendor_map is None:  # load it once
        with open('../vendor_mapping.json', 'r') as f:
            vendor_map = json.load(f)
            
    if isnan(enterprise):
        return None
    
    enterprise = str(int(enterprise)) # they are parsed as floats for some reason
    
    if enterprise in vendor_map:
        return vendor_map[enterprise]

    # logged
    logger.warning(f"Enterprise {enterprise} not found in vendor map. Add it to vendor_mapping.json.")
    return None


def convert_to_date(time: str) -> datetime.date:
    """
    Calculates the date the last boot took place from the snmpEngineTime field.
    :param time: the value of the snmpEngineTime field
    :return: the `datetime.date` representing the date of the last boot
    """
    days_since_reboot = re.search(r'(\d+)d', str(time)).group(1)

    reboot_date = datetime.date.today() - datetime.timedelta(days=int(days_since_reboot))

    return reboot_date.strftime('%Y-%m-%dT%H:%M:%S')


def build_query(query_input: list[tuple[str, str, str]]) -> str:
    conditions = ['m.datePublished > d.column2', 'm.shortVendorName = d.column3']
    selections = ['ip', 'm.cveId', 'm.shortVendorName']
    join_selections = ['cveLightId', 'cveId', 'datePublished', 'shortVendorName']
    
    # build selected columns
    if env.severity is not None and env.verbose_severity: # we can get the other attributes from the vector string
        selections.append('m.vectorString')
        join_selections.append('vectorString')
    elif env.severity is not None:
        selections.append('m.baseScore')
        # selections.append('m.baseSeverity')
        join_selections.append('baseScore')
        # join_selections.append('baseSeverity')
    
    # build conditions
    if env.routers_only: 
        # 0 = for sure related; 
        # 1 = did not have a description, so not sure; 
        # 2 = could not find keywords in the description
        conditions.append('m.relatedToRouters < 2')
        join_selections.append('relatedToRouters')

    if env.severity is not None:
        conditions.append(f'm.baseScore >= {env.severity}')

    full_input = ', '.join(['(' + ', '.join([f'\'{entry}\'' for entry in value]) + ')' for value in query_input])
        
    return f"""
       WITH date_assigner_pairs AS (VALUES {full_input})
        SELECT d.column1 as {', '.join(selections)}
        FROM date_assigner_pairs d
        JOIN (SELECT {', '.join(join_selections)} FROM cve_metadata NATURAL JOIN cve_vendor NATURAL JOIN vendors) m ON {' AND '.join(conditions)}
    """


def find_cves(scans: pd.DataFrame) -> pd.DataFrame | None:
    """
    Queries the database for any CVEs for the vendor that were published after the date of the last boot.
    :param scans: a row like in the input csv
    :param ip_cve_mapping: mutable dictionary that is updated with the found CVEs for the ip
    :return: nothing, it updates the dictionary instead
    """
    ips = scans['ip'].to_list()
    vendor_shortname = scans['enterprise'].apply(vendor_resolution).to_list()
    reboot_date = scans['snmpEngineTime'].apply(convert_to_date).to_list()

    if env.dry_run:
        return None

    values: list[tuple[str, str, str]] = [tup for tup in zip(ips, reboot_date, vendor_shortname) if tup[2] is not None]
    
    query = build_query(values)

    if env.explain_query:
        print(con.execute(f'EXPLAIN QUERY PLAN {query}').fetchall())
        print(query)
        
    print('Starting query')
    start = time.time()
    cve_list = (pd.read_sql_query(query, con))
                # .set_index('ip')) # need to do this to avoid weird stuff
    print(f'Queried and processed {len(ips)} items in {time.time() - start:.2f} seconds')

    return cve_list


def load_input() -> pd.DataFrame:
    """
    Reads all the scan results in the folder given as an argument into a single dataframe.
    :return: The dataframe with all the scan results.
    """
    import os
    files = [pd.read_csv(os.path.join(env.input, file)) for file in os.listdir(env.input)]
    
    return pd.concat(files).reset_index()


if __name__ == '__main__':
    df = load_input()
    
    if env.surf:
        df = df.loc[(df['asn_name'] == 'SURF B.V.') & (df['country'] == 'The Netherlands')]

    ip_cve_mapping: pd.DataFrame = find_cves(df)

    # dump results to a JSON file
    ip_cve_mapping.to_csv('../found_cves.csv')