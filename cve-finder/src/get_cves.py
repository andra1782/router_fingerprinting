import argparse
import logging
import os
import sqlite3
import time
from dataclasses import dataclass
from typing import Literal

import pandas as pd

"""
================================== GLOBALS =================================
"""

logger = logging.getLogger(__name__)
logging.basicConfig(filename='../get_cves.log', encoding='utf-8', format='%(asctime)s::%(levelname)s::%(message)s',
                    level=logging.INFO)

con = sqlite3.connect('../cve3.db')  # db connection
cur = con.cursor() 

vendor_map = None  # vendor names found using snmp differ from those in the db, so we define a mapping between them


@dataclass
class Env(argparse.Namespace):
    input: str
    # Initially, some vendor names from the snmp scans won't be in the `vendor_mapping.json`. First do a dry run,
    # which will report any missing mappings without doing any db operations. After you're sure all necessary mappings
    # were added, re-run the script for real
    dry_run: bool
    severity: float
    routers_only: bool
    surf: bool
    iberia: bool
    explain_query: bool
    input_tags: str
    exploitability_score: Literal['LOW', 'HIGH']


# get CLI args
parser = argparse.ArgumentParser()
parser.add_argument('-i', '--input', type=str, default='scan_results', help='path to the input csv file')
parser.add_argument('-n', '--dry-run', action='store_true', help='only log unknown vendors')
parser.add_argument('-s', '--severity', type=float, default=None,
                    help='include severity metrics with severity larger than the provided value in the output')
parser.add_argument('-e', '--exploitability-score', type=str, choices=['LOW', 'HIGH'], default=None, help='include only CVEs with this exploitability score')
parser.add_argument('-r', '--routers-only', action='store_true', help='only report CVEs related to routers')
parser.add_argument('--surf', action='store_true', help='keep only results from surf.nl')
parser.add_argument('-q', '--explain-query', action='store_true', help='explain the query that is being run')
parser.add_argument('--iberia', action='store_true', help='keep only results from the Iberian peninsula')
parser.add_argument('-it', '--input-tags', type=str, default='../bgp-tools/csvs', help='path to directory containing tags for asns')

env = Env(**vars(parser.parse_args()))

"""
=================================== METHODS ==================================
"""

def build_query() -> str:
    conditions = ['m.datePublished > d.snmpRebootDate', 'm.shortVendorName == d.enterprise', ]
    selections = ['m.cveId', 'm.shortVendorName', 'd.engineIDData', 'd.snmpEngineBoots', 'd.snmpRebootDate', 'd.asn', 'd.country']
    
    # build selected columns
    if env.severity is not None:
        selections.append('m.baseScore')
        
    if env.exploitability_score is not None:
        selections.append('m.exploitabilityScore')
    
    # build conditions
    if env.routers_only: 
        # 0 = for sure related; 
        # 1 = did not have a description, so not sure; 
        # 2 = could not find keywords in the description
        conditions.append('m.relatedToRouters < 2')

    if env.severity is not None:
        conditions.append(f'm.baseScore >= {env.severity}')
        
    if env.exploitability_score is not None:
        conditions.append(f'm.exploitabilityScore == "{env.exploitability_score}"')
        
    if env.surf:
        conditions.append('(d.asn_name == "SURF B.V." AND d.country == "The Netherlands")')
        
    if env.iberia:
        conditions.append('(d.country == "Spain" OR d.country == "Portugal")')

        
    return f"""
        SELECT {', '.join(selections)}
        FROM (SELECT * FROM scan_results WHERE 
            engineIDData NOT NULL
            AND snmpEngineBoots > 0
            AND enterprise NOT NULL
        GROUP BY engineIDData) d
        JOIN (SELECT * FROM cve_metadata NATURAL JOIN cve_vendor NATURAL JOIN vendors) m 
        ON {' AND '.join(conditions)}
    """


def find_cves() -> pd.DataFrame | None:
    """
    Queries the database for any CVEs for the vendor that were published after the date of the last boot.
    :param scans: a row like in the input csv
    :param engineid_cve_mapping: mutable dictionary that is updated with the found CVEs for the engineId
    :return: nothing, it updates the dictionary instead
    """
    query = build_query()

    if env.explain_query:
        print(query)
        print(con.execute(f'EXPLAIN QUERY PLAN {query}').fetchall())
        
    print('Starting query')
    start = time.time()
    cve_list = (pd.read_sql_query(query, con))
    print(f'Queried and processed items in {time.time() - start:.2f} seconds')

    return cve_list


def load_tags(dir: str) -> pd.DataFrame | None :
    try:

        if not os.path.isdir(dir):
            print(f"dir {dir} doesn't exist")
            return None

        dfs = []

        for filename in os.listdir(dir):
            if filename.endswith('.csv') and filename != "feeder.csv":
                filepath = os.path.join(dir, filename)
                try:
                    df = pd.read_csv(filepath, header=None, names=['ASN', 'Organization'])
                    df['source_file'] = filename.split(".")[0]
                    df = df.iloc[:, [2, 1, 0]]
                    df['ASN'] = df['ASN'].astype(str).str.upper().str.removeprefix("AS")
                    dfs.append(df)

                except Exception as e:
                    print(f"{filename} failed; error: {e}")

        if dfs:
            res = pd.concat(dfs, ignore_index=True)
            res.columns = ["Tag", "Organization Name", "ASN"]
            res.to_csv(r'tags.csv', index=None, sep=';', mode='w')
            return res
        else:
            print("No CSV files found.")
            return None
    except Exception as e:
        print(f"LOL{str(e)}")

if __name__ == '__main__':
    # if env.surf:
    #     df = df.loc[(df['asn_name'] == 'SURF B.V.') & (df['country'] == 'The Netherlands')]
    #     
    # if env.iberia:
    #     df = df.loc[(df['country'] == 'Spain') | (df['country'] == 'Portugal')]

    engineid_cve_mapping: pd.DataFrame = find_cves()

    # dump results to a JSON file
    engineid_cve_mapping.to_csv('../found_cves.csv', mode='w', index=False)

    if env.input_tags:
        print("Adding tags to output")
        tags_df = load_tags(env.input_tags)

        tags_df.to_csv('tags.csv', index=False)

        original_length = len(engineid_cve_mapping)

        if tags_df is not None and engineid_cve_mapping is not None:
            tags_df['ASN'] = tags_df['ASN'].astype(str).str.strip()
            engineid_cve_mapping['asn'] = engineid_cve_mapping['asn'].astype(str).str.strip()


            print("Aggregating tags per asn")
            tags_agg = tags_df.groupby('ASN')['Tag'].apply(lambda x: ';'.join(sorted(set(x)))).reset_index()

            print("Merging aggregated tags")

            merged = pd.merge(
                engineid_cve_mapping,
                tags_agg,
                left_on='asn',
                right_on='ASN',
                how='left'
            ).drop(columns=['ASN'])

            merged['Tag'] = merged['Tag'].fillna('UNKNOWN')

            merged.rename(columns={'Tag': 'tag'}, inplace=True)

            columns_order = ['cveId', 'shortVendorName', 'engineIDData', 'snmpEngineBoots', 'snmpRebootDate', 'asn', 'tag', 'country']
            if 'baseScore' in merged.columns:
                columns_order += ['baseScore']
            merged = merged[columns_order]
                        
            output_with_tags_path = '../found_cves_with_tags.csv'
            merged.to_csv(output_with_tags_path, index=False)

            print(f"Saved results with tags to {output_with_tags_path}")
            print(f"Original CVE rows: {original_length}")
            print(f"Final tagged rows: {len(merged)}")